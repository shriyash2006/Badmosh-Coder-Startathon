================================================================================
  BADMOSH CODERS — TECHNICAL REPORT
  Duality AI GHR 2.0 Hackathon 2025
  Offroad Semantic Scene Segmentation
================================================================================

TEAM
────────────────────────────────────────────────────────────────────────────────
  Team Name   : Badmosh Coders
  Event       : Duality AI GHR 2.0 Hackathon 2025
  Task        : Semantic segmentation of synthetic offroad desert terrain
  Platform    : Falcon Digital Twin
  Hardware    : NVIDIA T4 GPU (Google Colab)

================================================================================
  1. PROBLEM STATEMENT
================================================================================

Segment every pixel of synthetic desert terrain images into one of 11 semantic
classes. Images are generated by the Falcon digital twin platform, representing
realistic offroad environments with significant class imbalance — Sky and
Landscape dominate, while Flowers, Logs, and Ground Clutter are extremely rare.

================================================================================
  2. DATASET
================================================================================

  Source          : Duality AI Falcon Platform (synthetic digital twin)
  Training images : 2,857
  Validation imgs : 317
  Test images     : 1,002
  Resolution      : 952 × 532 (model input, divisible by 14 for ViT patches)

  Raw mask pixel values → class indices:
  ┌─────────────┬────┬─────────────────┐
  │ Pixel Value │ ID │ Class Name       │
  ├─────────────┼────┼─────────────────┤
  │           0 │  0 │ Background       │
  │         100 │  1 │ Trees            │
  │         200 │  2 │ Lush Bushes      │
  │         300 │  3 │ Dry Grass        │
  │         500 │  4 │ Dry Bushes       │
  │         550 │  5 │ Ground Clutter   │
  │         600 │  6 │ Flowers          │
  │         700 │  7 │ Logs             │
  │         800 │  8 │ Rocks            │
  │        7100 │  9 │ Landscape        │
  │       10000 │ 10 │ Sky              │
  └─────────────┴────┴─────────────────┘

================================================================================
  3. MODEL ARCHITECTURE
================================================================================

3.1  Backbone — DINOv2 ViT-B/14
────────────────────────────────────────────────────────────────────────────────
  Pre-trained on 142M images via self-supervised DINO v2 objective.
  Patch size: 14×14 pixels → 68×38 = 2,584 patch tokens at 952×532 input.
  Embedding dimension: 768.
  Total parameters: 86,580,480.
  Unfrozen for fine-tuning: blocks 6–11 → 42,536,448 parameters.
  Fine-tuning learning rate: 3e-5 with gradient clipping at max_norm=1.0.

  The frozen early blocks (0–5) preserve low-level texture features learned
  from the massive pretraining set. The unfrozen later blocks adapt to the
  desert terrain domain.

3.2  Multi-Scale Feature Extraction
────────────────────────────────────────────────────────────────────────────────
  Forward hooks capture intermediate patch tokens from 4 transformer blocks:

  Stage 0 → Block 2  : Early features (edges, textures, fine detail)
  Stage 1 → Block 5  : Mid-level features (local structures, bark, rock face)
  Stage 2 → Block 8  : High-level features (semantic parts, tree canopy)
  Stage 3 → Block 11 : Final features (global context, sky region, landscape)

  All stages share the same spatial resolution (68×38 patch grid) but carry
  information at different levels of semantic abstraction.

3.3  FPN Decoder Head — 4,297,291 Parameters
────────────────────────────────────────────────────────────────────────────────
  Feature Pyramid Network decoder fuses the 4 stages top-down:

  Step 1: Lateral projections
    Each stage projected from 768-dim → 256-dim via:
    Conv2d(768, 256, 1×1) → BatchNorm2d → GELU

  Step 2: Top-down pathway
    Starting from deepest stage (Block 11):
      P3 = lateral[3]
      P2 = td_conv(lateral[2] + upsample(P3))
      P1 = td_conv(lateral[1] + upsample(P2))
      P0 = td_conv(lateral[0] + upsample(P1))
    Where td_conv = Conv2d(256,256,3) → BatchNorm2d → GELU

    Deep semantic context flows down and guides interpretation of shallow
    texture features — producing sharper class boundaries.

  Step 3: Progressive 8× upsampling decoder
    up1: Upsample(2×) → Conv(256→256) → Conv(256→256)   [68×38 → 136×76]
    up2: Upsample(2×) → Conv(256→128) → Conv(128→128)   [136×76 → 272×152]
    up3: Upsample(2×) → Conv(128→64)  → Conv(64→64)     [272×152 → 544×304]

  Step 4: Final bilinear interpolation to 952×532

  Step 5: Dropout2d(p=0.1) + Conv2d(64, 11, 1×1) classifier

================================================================================
  4. TRAINING STRATEGY
================================================================================

4.1  Loss Function
────────────────────────────────────────────────────────────────────────────────
  Combined: 0.6 × FocalLoss + 0.4 × DiceLoss

  Focal Loss (γ=2, label_smoothing=0.05):
    L_focal = (1 - p_t)^γ × CE(logits, targets)
    γ=2 down-weights easy pixels (high p_t), forcing the model to
    concentrate gradient budget on hard misclassifications like Logs and
    Ground Clutter. Label smoothing prevents overconfidence on dominant
    classes.

  Dice Loss:
    L_dice = 1 - (2 × intersection + ε) / (union + ε)
    Optimizes class overlap directly, independent of pixel count.
    Averages equally across all 11 classes regardless of frequency.

  Class Weights (applied to Focal Loss):
    Background    : 0.4   (suppress)
    Trees         : 1.0
    Lush Bushes   : 1.2
    Dry Grass     : 1.0
    Dry Bushes    : 2.0
    Ground Clutter: 3.0   (boost rare class)
    Flowers       : 4.0   (boost very rare class)
    Logs          : 4.0   (boost very rare class)
    Rocks         : 2.0
    Landscape     : 0.4   (suppress dominant class)
    Sky           : 0.4   (suppress dominant class)

4.2  Optimizer & Scheduler
────────────────────────────────────────────────────────────────────────────────
  Optimizer: AdamW (weight_decay=1e-4)

  Differential learning rates:
    Backbone (unfrozen blocks 6–11) : lr = 3e-5
    FPN head                        : lr = 3e-4

  Scheduler: OneCycleLR
    max_lr       = [3e-5, 3e-4]
    pct_start    = 0.3   (ramp up for first 30% of steps)
    anneal_strategy = 'cos'
    div_factor   = 10
    final_div_factor = 1000
    Steps per epoch: 1,429 (2857 images / batch_size 2)

  Gradient clipping: max_norm=1.0
  AMP (automatic mixed precision): enabled on CUDA

4.3  Data Augmentation (training only)
────────────────────────────────────────────────────────────────────────────────
  All spatial augmentations applied identically to image AND mask:
    - Random horizontal flip (p=0.5)
    - Random vertical flip (p=0.5)
    - Random rotation ±10° (p=0.5)
    - RandomResizedCrop: scale (0.6–1.0), ratio (3/4–4/3) (p=0.5)

  Image-only augmentations:
    - ColorJitter: brightness=0.4, contrast=0.4, saturation=0.4, hue=0.15
    - Random grayscale (p=0.15)
    - Gaussian blur, kernel 3/5/7 (p=0.5)

  Normalization: ImageNet mean/std
    mean = [0.485, 0.456, 0.406]
    std  = [0.229, 0.224, 0.225]

4.4  Inference — Test-Time Augmentation (TTA)
────────────────────────────────────────────────────────────────────────────────
  Two forward passes averaged:
    1. Original image
    2. Horizontally flipped image (prediction un-flipped before averaging)

  Averaged at softmax probability level for better calibration.
  Free +1–2 mIoU boost at zero training cost.

================================================================================
  5. METRICS
================================================================================

5.1  mIoU (Mean Intersection over Union)
────────────────────────────────────────────────────────────────────────────────
  For each class c:
    IoU_c = |pred_c ∩ gt_c| / |pred_c ∪ gt_c|

  Classes with no GT pixels (union=0) excluded from per-image mean.
  Final mIoU = mean over all valid (image, class) pairs.

5.2  mAP@50 (Mean Average Precision at IoU=0.50)
────────────────────────────────────────────────────────────────────────────────
  Adapts the COCO-style mAP to semantic segmentation at image level:

  For each class c and image:
    - Confidence score = max softmax probability for class c
    - TP if class c present in GT AND IoU_c >= 0.50
    - FP if predicted but IoU_c < 0.50 OR not in GT
    - FN if in GT but not predicted

  Per-class AP computed as area under Precision-Recall curve.
  mAP@50 = mean AP across all 11 classes at threshold 0.50.

5.3  mAP@50:95
────────────────────────────────────────────────────────────────────────────────
  mAP averaged over IoU thresholds: 0.50, 0.55, 0.60, ..., 0.95 (10 values).
  Standard COCO benchmark metric.

================================================================================
  6. RESULTS
================================================================================

  Validation set (317 images, epoch 15):
  ┌─────────────────┬────────┐
  │ Metric          │ Score  │
  ├─────────────────┼────────┤
  │ Val mIoU        │ 0.5668 │
  │ mAP@50          │  TBD   │
  │ mAP@50:95       │  TBD   │
  └─────────────────┴────────┘

  Validation per-class IoU (epoch 2 sample):
  ┌────┬─────────────────┬────────┐
  │ ID │ Class           │  IoU   │
  ├────┼─────────────────┼────────┤
  │  0 │ Background      │ 0.0000 │
  │  1 │ Trees           │ 0.6881 │
  │  2 │ Lush Bushes     │ 0.5881 │
  │  3 │ Dry Grass       │ 0.6322 │
  │  4 │ Dry Bushes      │ 0.3961 │
  │  5 │ Ground Clutter  │ 0.2858 │
  │  6 │ Flowers         │ 0.4308 │
  │  7 │ Logs            │ 0.1916 │
  │  8 │ Rocks           │ 0.3251 │
  │  9 │ Landscape       │ 0.3983 │
  │ 10 │ Sky             │ 0.9661 │
  └────┴─────────────────┴────────┘

  Training timeline (T4 GPU):
    ~12 minutes per epoch at batch_size=2, resolution 952×532
    15 epochs completed in ~3 hours
    Best checkpoint: epoch 15

================================================================================
  7. FILES
================================================================================

  train_fpn.py
    Main training script. Contains DINOv2MultiScale, FPNSegmentationHead,
    FocalLoss, DiceLoss, MaskDataset, checkpoint save/load, OneCycleLR,
    TTA validation, and Google Drive auto-backup.

  test.py
    Full evaluation pipeline. Computes mIoU, mAP@50, mAP@50:95 per class.
    Generates: summary card, IoU bar chart, mAP chart, class tiles,
    IoU distribution histogram, side-by-side prediction grids,
    and test_results.txt.

  index.html
    Single-file hackathon presentation website. Dark terminal aesthetic.
    Sections: Overview, Architecture, Per-class IoU bars, Training curves,
    Improvements, Team. Self-contained (no external files needed).

================================================================================
  8. DEPENDENCIES
================================================================================

  torch >= 2.0
  torchvision >= 0.15
  numpy
  matplotlib
  Pillow
  tqdm

  DINOv2 loaded via torch.hub:
    torch.hub.load("facebookresearch/dinov2", "dinov2_vitb14")

================================================================================
  Badmosh Coders · Duality AI GHR 2.0 Hackathon 2025
  DINOv2 ViT-B/14 · FPN Decoder · 11 Classes · Falcon Platform
================================================================================
