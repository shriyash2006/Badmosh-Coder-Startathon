<div align="center">

```
                                                                                                          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—
                                                                                                          â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘
                                                                                                          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘
                                                                                                          â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘
                                                                                                          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘
                                                                                                          â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â• â•šâ•â•     â•šâ•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•
                                                                                                                                   C O D E R S
```

# ğŸœï¸ Offroad Semantic Segmentation
### Duality AI GHR 2.0 Hackathon 2025

[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org)
[![DINOv2](https://img.shields.io/badge/DINOv2-ViT--B/14-0068C1?style=for-the-badge)](https://github.com/facebookresearch/dinov2)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)

*Pixel-perfect desert terrain understanding using self-supervised vision transformers*

</div>

---

## ğŸ“Š Results at a Glance

| Metric | Score |
|--------|-------|
| **Val mIoU** | `0.5668` |
| **mAP@50** | â€” |
| **mAP@50:95** | â€” |
| **Backbone** | DINOv2 ViT-B/14 (86M params) |
| **Head** | FPN Decoder (4.3M params) |
| **Training Images** | 2,857 |
| **Test Images** | 1,002 |
| **Classes** | 11 |
| **Epochs** | 15 |
| **GPU** | NVIDIA T4 |

---

## ğŸ—ï¸ Architecture

```
Input Image (952Ã—532Ã—3)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DINOv2 ViT-B/14 Backbone        â”‚
â”‚              86M Parameters             â”‚
â”‚                                         â”‚
â”‚  Block 2  â”€â”€â–º Stage 0 (texture/edges)   â”‚
â”‚  Block 5  â”€â”€â–º Stage 1 (local structure) â”‚
â”‚  Block 8  â”€â”€â–º Stage 2 (semantic parts)  â”‚
â”‚  Block 11 â”€â”€â–º Stage 3 (global context)  â”‚
â”‚                                         â”‚
â”‚  Blocks 6â€“11 fine-tuned @ lr=3e-5       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚  4Ã— (B, 2584, 768) feature maps
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           FPN Decoder Head              â”‚
â”‚              4.3M Parameters            â”‚
â”‚                                         â”‚
â”‚  Lateral projections (768 â†’ 256 each)   â”‚
â”‚  Top-down fusion (deep guides shallow)  â”‚
â”‚  3Ã— Progressive upsampling (2Ã—, 2Ã—, 2Ã—) â”‚
â”‚  Dropout(0.1) + 1Ã—1 Classifier          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚  (B, 11, H, W)
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Bilinear Interpolation to 952Ã—532    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
     Output Mask (11 classes, per pixel)
```

---

## ğŸ¨ Class Definitions

| ID | Class | Color | Weight |
|----|-------|-------|--------|
| 0 | Background | â¬› `#0F0F0F` | 0.4Ã— |
| 1 | Trees | ğŸŸ© `#228B22` | 1.0Ã— |
| 2 | Lush Bushes | ğŸŸ¢ `#00D26E` | 1.2Ã— |
| 3 | Dry Grass | ğŸŸ¡ `#D2B478` | 1.0Ã— |
| 4 | Dry Bushes | ğŸŸ« `#A06428` | 2.0Ã— |
| 5 | Ground Clutter | ğŸ”˜ `#787888` | 3.0Ã— |
| 6 | Flowers | ğŸŒ¸ `#FF64B4` | 4.0Ã— |
| 7 | Logs | ğŸŸ¤ `#5A3719` | 4.0Ã— |
| 8 | Rocks | âšª `#B4AFAA` | 2.0Ã— |
| 9 | Landscape | ğŸŸ§ `#C2AA6E` | 0.4Ã— |
| 10 | Sky | ğŸ”µ `#64B4F0` | 0.4Ã— |

---

## ğŸ”§ Key Improvements

### 1. ğŸ§  FPN Multi-Scale Decoder
Taps into 4 intermediate DINOv2 transformer blocks simultaneously. A top-down pathway merges deep semantic context with shallow texture features â€” producing sharper class boundaries than a single-layer decoder.

### 2. ğŸ¯ Focal Loss (Î³=2)
Down-weights easy, well-classified pixels so the gradient budget concentrates on hard misclassifications like Logs and Ground Clutter. Combined with per-class weights for rare classes.

### 3. ğŸ”“ Deeper Backbone Fine-Tuning
Blocks 6â€“11 unfrozen (doubled from original 9â€“11 only). Gradient clipping at `max_norm=1.0` keeps training stable while allowing richer domain adaptation to desert terrain.

### 4. ğŸ“ˆ OneCycleLR Scheduler
LR ramps up for the first 30% of training then anneals sharply â€” finds a better minimum in fewer epochs vs CosineAnnealingLR.

### 5. ğŸ”„ Test-Time Augmentation (TTA)
Validation averages original and horizontally-flipped predictions. Zero training cost, free +1â€“2 mIoU at inference time.

### 6. ğŸ² Richer Augmentation
Random rotation (Â±10Â°), resized crop (60â€“100% zoom), Gaussian blur, color jitter, and random grayscale â€” all synchronized between image and mask.

---

## âš™ï¸ Training Configuration

```python
# Backbone
backbone     = "DINOv2 ViT-B/14"
unfrozen     = "blocks 6â€“11"
embed_dim    = 768

# Head
fpn_dim      = 256
decoder_ups  = 3   # 3Ã— progressive 2Ã— upsample

# Training
epochs       = 15
batch_size   = 2
lr_head      = 3e-4
lr_backbone  = 3e-5
weight_decay = 1e-4
optimizer    = "AdamW"
scheduler    = "OneCycleLR (pct_start=0.3)"
grad_clip    = 1.0
loss         = "0.6 Ã— FocalLoss(Î³=2) + 0.4 Ã— DiceLoss"
label_smooth = 0.05
resolution   = "952 Ã— 532"
amp          = True
```

---

## ğŸš€ Setup & Usage

### Installation
```bash
git clone https://github.com/pratyushmathur05/badmosh-coders.git
cd badmosh-coders
pip install torch torchvision tqdm matplotlib pillow
```

### Training
```bash
# Update paths in train_fpn.py:
# TRAIN_DIR, VAL_DIR, RUNS_DIR

python train_fpn.py
```

### Testing
```bash
# Update paths in test.py:
# TEST_DIR, RUNS_DIR, OUTPUT_DIR

python test.py
```

### Outputs
```
test_outputs/
â”œâ”€â”€ summary_card.png             â† mIoU + mAP gauges overview
â”œâ”€â”€ iou_bar_chart.png            â† per-class IoU horizontal bars
â”œâ”€â”€ map_chart.png                â† per-class AP@50
â”œâ”€â”€ class_performance_tiles.png  â† circular gauge per class
â”œâ”€â”€ iou_distribution.png         â† histogram of per-image scores
â”œâ”€â”€ predictions/
â”‚   â””â”€â”€ *_pred.png               â† input | prediction | GT grids
â””â”€â”€ test_results.txt             â† full numeric results
```

---

## ğŸ“ Repository Structure

```
badmosh-coders/
â”œâ”€â”€ train_fpn.py          â† main training script (FPN + DINOv2)
â”œâ”€â”€ test.py               â† inference + mIoU + mAP50 evaluation
â”œâ”€â”€ index.html            â† hackathon presentation website
â”œâ”€â”€ document.txt          â† full technical report
â””â”€â”€ README.md
```

---

## ğŸ† Hackathon

**Event:** Duality AI GHR 2.0 Hackathon 2025
**Team:** Badmosh Coders
**Platform:** Falcon Digital Twin (synthetic desert data)
**Stack:** PyTorch Â· DINOv2 Â· Google Colab T4 Â· Falcon Platform

---

<div align="center">

**Built with â—ˆ by Badmosh Coders**
*DINOv2 ViT-B/14 Â· FPN Decoder Â· 11 Classes Â· Synthetic Desert Data*

</div>
